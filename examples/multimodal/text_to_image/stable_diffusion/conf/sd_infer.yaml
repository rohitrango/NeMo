name: stable-diffusion-train

infer:
  unconditional_guidance_scale: 7.5
  num_images_per_prompt: 1000
  batch_size: 16
  height: 512
  width: 512
  down_factor: 8
  inference_steps: 50
  sampler_type: 'DPM'
  eta: 0
  output_type: 'pil'
  save_to_file: True
  out_path: 'stable-diffusion'
  seed: 123
  prompts:
    - 'A photo of a Shiba Inu dog with a backpack riding a bike. It is wearing sunglasses and a beach hat.'
    - 'A cute corgi lives in a house made out of sushi.'
    - 'A high contrast portrait of a very happy fuzzy panda dressed as a chef in a high end kitchen making dough. There is a painting of flowers on the wall behind him.'
    - 'A brain riding a rocketship heading towards the moon.'

trainer:
  devices: 1
  num_nodes: 1
  accelerator: gpu
  precision: 16
  logger: False # logger provided by exp_manager

# model:
#   first_stage_config:
#     from_pretrained: /opt/nemo-aligner/checkpoints/sdv1_4/vae.bin

model:
  restore_from_path: ""
  precision: ${trainer.precision}
  # specify micro_batch_size, global_batch_size, and model parallelism
  # gradient accumulation will be done automatically based on data_parallel_size
  micro_batch_size: 16 # limited by GPU memory
  global_batch_size: 16 # will use more micro batches to reach global batch size
  native_amp_init_scale: 65536.0 # Init scale for grad scaler used at fp16


  linear_start: 0.00085
  linear_end: 0.012
  num_timesteps_cond: 1
  log_every_t: 200
  timesteps: 1000
  first_stage_key: images
  cond_stage_key: captions # txt for cifar, caption for pbss
  image_size: 64
  channels: 4
  cond_stage_trainable: false
  conditioning_key: crossattn # check
  monitor: val/loss_simple_ema
  scale_factor: 0.18215
  use_ema: False
  scale_by_std: False
  ckpt_path:
  ignore_keys: []
  parameterization: eps
  clip_denoised: True
  load_only_unet: False
  cosine_s: 8e-3
  given_betas:
  original_elbo_weight: 0
  v_posterior: 0
  l_simple_weight: 1
  use_positional_encodings: False
  learn_logvar: False
  logvar_init: 0
  beta_schedule: linear
  loss_type: l2

  concat_mode: True
  cond_stage_forward:
  text_embedding_dropout_rate: 0.1
  fused_opt: True
  inductor: False
  inductor_cudagraphs: False
  capture_cudagraph_iters: -1 # -1 to disable
  channels_last: True

  unet_config:
    _target_: nemo.collections.multimodal.modules.stable_diffusion.diffusionmodules.openaimodel.UNetModel
    # from_pretrained: #/ckpts/nemo-v1-2.ckpt
    from_pretrained: /opt/nemo-aligner/checkpoints/sdv1_4/unet_nemo.ckpt
    from_NeMo: False #Must be specified when from pretrained is not None, False means loading unet from HF ckpt
    image_size: 32 # unused
    in_channels: 4
    out_channels: 4
    model_channels: 320
    attention_resolutions:
      - 4
      - 2
      - 1
    num_res_blocks: 2
    channel_mult:
    - 1
    - 2
    - 4
    - 4
    num_heads: 8
    use_spatial_transformer: true
    transformer_depth: 1
    context_dim: 768
    use_checkpoint: False
    legacy: False
    use_flash_attention: True
    unet_precision: fp32
    resblock_gn_groups: 32
    use_te_fp8: False

  first_stage_config:
    _target_: nemo.collections.multimodal.models.text_to_image.stable_diffusion.ldm.autoencoder.AutoencoderKL
    from_pretrained: /opt/nemo-aligner/checkpoints/sdv1_4/vae.bin
    # from_pretrained: /ckpts/vae.bin
    embed_dim: 4
    monitor: val/rec_loss
    ddconfig:
      double_z: true
      z_channels: 4
      resolution: 256  #Never used
      in_channels: 3
      out_ch: 3
      ch: 128
      ch_mult:
      - 1
      - 2
      - 4
      - 4
      num_res_blocks: 2
      attn_resolutions: [ ]
      dropout: 0.0
    lossconfig:
      target: torch.nn.Identity

  cond_stage_config:
    _target_: nemo.collections.multimodal.modules.stable_diffusion.encoders.modules.FrozenCLIPEmbedder
    version: openai/clip-vit-large-patch14
    device: cuda
    max_length: 77
  #    _target_: nemo.collections.multimodal.modules.stable_diffusion.encoders.modules.FrozenMegatronCLIPEmbedder
  #    restore_from_path: /ckpts/openai-old.nemo
  #    device: cuda
  #    freeze: True
  #    layer: "last"

  # miscellaneous
  seed: 1234
  resume_from_checkpoint: null # manually set the checkpoint file to load from
  apex_transformer_log_level: 30 # Python logging level displays logs with severity greater than or equal to this
  gradient_as_bucket_view: True # PyTorch DDP argument. Allocate gradients in a contiguous bucket to save memory (less fragmentation and buffer memory)
  ddp_overlap: False # True for using PyTorch DDP overlap.

  optim:
    name: fused_adam
    lr: null
    weight_decay: 0.
    betas:
      - 0.9
      - 0.999
    sched:
      name: WarmupHoldPolicy
      warmup_steps: 10000
      hold_steps: 10000000000000 # Incredibly large value to hold the lr as constant

  # Nsys profiling options
  nsys_profile:
    enabled: False
    start_step: 10  # Global batch to start profiling
    end_step: 10 # Global batch to end profiling
    ranks: [ 0 ] # Global rank IDs to profile
    gen_shape: False # Generate model and kernel details including input shapes

  data:
    num_workers: 16
    synthetic_data: False # dataset_path and local_root_path can be empty when using synthetic data
    synthetic_data_length: 10000
    train:
      dataset_path:
        - /datasets/coyo/wdinfo/coyo-700m/wdinfo-selene.pkl
      augmentations:
        resize_smallest_side: 512
        center_crop_h_w: 512, 512
        horizontal_flip: False
      filterings:

    webdataset:
      infinite_sampler: False
      local_root_path: /datasets/coyo